#!/bin/bash
# Created by the ARC-TS SLURM job script generator for use on Great Lakes
# Thu Apr 14 2022 10:24:48 GMT-0400 (EDT)

# The name of the job:
#SBATCH --job-name="1d global search"

# Send yourself an email when the job:
# aborts abnormally (fails)
#SBATCH --mail-type=FAIL
# ends successfully
#SBATCH --mail-type=END

# Use this email address:
#SBATCH --mail-user=aaronabk@umich.edu

# Maximum number of tasks/CPU cores used by the job:
#SBATCH --nodes=1
# ntasks is the important one
# --ntasks-per-node=36
#SBATCH --cpus-per-task=1

# Total memory allocated for the job:
## 5GB/cpu is the basic share
#SBATCH --mem-per-cpu=2GB

# The maximum running time of the job in days-hours:mins:sec
# For run level 1:
# --time=0-0:02:00
# For run level 3:
# --time=1-4:00:00

# The account which this job should run under:
#SBATCH --account="ionides0"

# Partition for the job:
#SBATCH -p standard

## Important variables ##
# R file to run
R_file="1d_global_search.R"

# The modules to load:
module load R/4.4.0

# Useful for debugging:
echo "Running on $SLURM_JOB_NODELIST"
echo "Running in $(pwd)"
echo "Using run level $run_level"
 
### R commands ###
echo "Running 1d_global_search/$R_file..."
R CMD BATCH --no-restore --no-save \
  1d_global_search/$R_file 1d_global_search/$R_file"out"

