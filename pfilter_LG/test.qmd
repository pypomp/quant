---
title: 'Test the Linear Gaussian Model: A comparison with Kalman-filtering algorithm'
jupyter: python3
---


```{python}
#| label: imports
#| echo: false
import jax
import time
import pypomp
import unittest
import tracemalloc
import jax.numpy as np
import numpy as onp
import pandas as pd
import datetime

from tabulate import tabulate

import pykalman
import seaborn as sns
import matplotlib.pyplot as plt
import jax.scipy.special

from tqdm import tqdm
from pypomp.mop import mop
from pypomp.pfilter import pfilter
from pypomp.fit import fit
from pypomp.internal_functions import _mop_internal
from pypomp.internal_functions import _pfilter_internal
from pypomp.internal_functions import _pfilter_internal_mean
from pypomp.internal_functions import _fit_internal
from pypomp.internal_functions import _perfilter_internal
from importlib.metadata import version

```



```{python}
#| label: run_level
#| echo: false
run_level = 2
N = [10, 50, 100][run_level]
J = [10,1000,10000][run_level]
N_reps = [3,10,100][run_level]
N_values = [[10,50],[100,500],[100,500]][run_level]
J_values = [[5,10,20,50],[5,10,100,1000],[10,100,1000,10000]][run_level]
```

Testing pypomp `{python} version('pypomp')` on `{python} datetime.date.today().strftime("%Y-%m-%d")` at run level `{python} run_level` (0 is for debugging, 2 is full-length).

# Model Setup

We consider a linear Gaussian dynamic system:
\begin{align}
    X_n &= A X_{n-1} + W_n \\
    \text{where} \quad
    X_n &\text{ is the current state vector.} \nonumber \\
    A &\text{ is the state transition matrix.} \nonumber \\
    W_n &\sim \mathcal{N}(0, Q) \text{ is the process noise, normally distributed with mean 0 and covariance } Q. \nonumber \\
    Y_n &= C X_n + V_n \\
    \text{where} \quad
    Y_n &\text{ is the current observation vector.} \nonumber \\
    C &\text{ is the observation matrix.} \nonumber \\
    V_n &\sim \mathcal{N}(0, R) \text{ is the observation noise, normally distributed with mean 0 and covariance} R. \nonumber
\end{align}


(1) Set N (length of time series) to be `{python} N` and generate linear Gaussian states and observations:

```{python}
#| label: LG-POMP model
#| code-fold: true
def get_thetas(theta):
    A = theta[0:4].reshape(2, 2)
    C = theta[4:8].reshape(2, 2)
    Q = theta[8:12].reshape(2, 2)
    R = theta[12:16].reshape(2, 2)
    return A, C, Q, R

def transform_thetas(A, C, Q, R):
    return np.concatenate([A.flatten(), C.flatten(), Q.flatten(), R.flatten()])


fixed = False
key = jax.random.PRNGKey(111)
angle = 0.2
angle2 = angle if fixed else -0.5
A = np.array([[np.cos(angle2), -np.sin(angle)],
             [np.sin(angle), np.cos(angle2)]])
C = np.eye(2)
Q = np.array([[1, 1e-4],
             [1e-4, 1]]) # 100
R = np.array([[1, .1],
            [.1, 1]]) #/ 10
     
theta = transform_thetas(A, C, Q, R)

def generate_data(N, key):
    xs = []
    ys = []
    x = np.ones(2)
    for i in tqdm(range(N)):
        key, subkey = jax.random.split(key)
        x = jax.random.multivariate_normal(key=subkey, mean=A @ x, cov=Q)
        key, subkey = jax.random.split(key)
        y = jax.random.multivariate_normal(key=subkey, mean=C @ x, cov=R)
        xs.append(x)
        ys.append(y)
    xs = np.array(xs)
    ys = np.array(ys)
    return xs, ys, key


def custom_rinit(theta, J, covars=None):
    return np.ones((J, 2))


def custom_rproc(state, theta, key, covars=None):
    A, C, Q, R = get_thetas(theta)
    key, subkey = jax.random.split(key)
    return jax.random.multivariate_normal(key=subkey,
                                          mean=A @ state, cov=Q)
					  

def custom_dmeas(y, preds, theta):
    A, C, Q, R = get_thetas(theta)
    return jax.scipy.stats.multivariate_normal.logpdf(y, preds, R)


rinit = custom_rinit
rproc = custom_rproc
dmeas = custom_dmeas
rprocess = jax.vmap(custom_rproc, (0, None, 0, None))
dmeasure = jax.vmap(custom_dmeas, (None, 0, None))
rprocesses = jax.vmap(custom_rproc, (0, 0, 0, None))
dmeasures = jax.vmap(custom_dmeas, (None, 0, 0))
```

```{python}
#| label: parameter transformation
#| echo: false
def logmeanexp(x):
   x_array = np.array(x)
   x_max = np.max(x_array)
   log_mean_exp = np.log(np.mean(np.exp(x_array - x_max))) + x_max
   return log_mean_exp
   
```

Set J=`{python} J` particles and compare the estimated log-likelihood between Kalman filtering and the log-mean-exponential computed over `{python} N_reps` replications for various methods, including classical particle filtering and MOP, etc.

```{python}
#| label: sim and pykalman
xs, ys, key = generate_data(N, key)
kf = pykalman.KalmanFilter(
    transition_matrices=A, observation_matrices=C, 
    transition_covariance=Q, observation_covariance=R)
print("kf loglik =", kf.loglikelihood(ys))
```

```{python}
#| label: pfilter test
loglike = []
for i in range(N_reps):  
    key, subkey = jax.random.split(key)
    pfilter_val = -_pfilter_internal(
        theta, ys, J = J, rinit = rinit,
	rprocess = rprocess, dmeasure = dmeasure,
	covars = None, key= subkey, thresh = -1)
    loglike.append(pfilter_val)

loglike_ = np.array(loglike)

print("Logmeanexp of Particle Filtering =", logmeanexp(loglike))
print("difference between Kalman-Filtering and logmeanexp of Particle Filtering =",
    kf.loglikelihood(ys) - (logmeanexp(loglike)))
```

By calculating the difference between Kalman-filtering and Paticle Filtering algorithm, we discovered that the value of difference can be less than 0.1, indicating that we get a reasonable inference result from the pfilter algorithm.

Next, we test the ouput of the MOP algorithm. The pypomp MOP algorithm is set to be $\phi = \theta$. Under this case, the MOP algorithm should be equivalent with the particle filter algorithm, and have the same output values when setting the same random seeds.

```{python}
#| label: test MOP at different alphas
#| output: asis
alphas = [0, 0.1, 0.3, 0.6, 0.9, 1]
results = []


key = jax.random.PRNGKey(0)  # Use a fixed seed for reproducibility
subkeys = jax.random.split(key, 100)  # Pre-generate 100 keys

for alpha in alphas:
    loglike_mop = []
    for i, subkey in enumerate(subkeys):  
        mop_val = -_mop_internal(theta, ys, J=J, rinit=rinit,
	    rprocess=rprocess, dmeasure=dmeasure,
	    covars=None, key=subkey, alpha=alpha)
        loglike_mop.append(mop_val)
    loglike_mop = np.array(loglike_mop)
    logmeanexp_val = logmeanexp(loglike_mop)
    difference = kf.loglikelihood(ys) - logmeanexp_val
    
    results.append((alpha, logmeanexp_val, difference))
#    print(f"Alpha: {alpha}, Logmeanexp: {logmeanexp_val}, Difference: {difference}")


# Use the same random key to test the particle filter output
loglike_pf = []
for i, subkey in enumerate(subkeys):  
    pfilter_val = -_pfilter_internal(
        theta, ys, J = J, rinit = rinit,
	rprocess = rprocess, dmeasure = dmeasure,
	covars = None, key= subkey, thresh = -1)
    loglike_pf.append(pfilter_val)


alpha_table=tabulate(results, tablefmt="grid", headers=("alpha","pf","kalman-pf"))
print(alpha_table)
```

Consistency: For $\alpha = 0$ and $\alpha = 1$, the MOP logmeanexp is `{python} print(round(results[0][1],3))` and `{python} print(round(results[5][1],3))` respectively.
For intermediate $\alpha$ values, the MOP logmeanexp slightly deviates to `{python} print(round(results[3][1],3))`
The Logmeanexp under $\alpha = 0$ and $\alpha = 1$ are closer to the Kalman-Filtering results and the particle filter logmeanexp outputs.


### Test the Linear Gaussian Model: How estimate logllikehood difference and running time varys among different N and J

```{python}
#| label: N-J-test

NJresults = []
key = jax.random.PRNGKey(112)

for N_val in N_values:
    for J_val in J_values:
        print(f"Running with N={N_val}, J={J_val}...")
        
        xs, ys, key = generate_data(N_val, key)
        pf_loglik_arr = []
        mop_loglik_arr = []
        elapsed_time1_arr = []
        elapsed_time2_arr = []
        
        for i in range(N_reps):  
            start_time = time.time()
            pf_val = -pfilter(J = J_val,
	        rinit = rinit, rprocess = rprocess, dmeasure = dmeasure,
		theta = theta, ys = ys, thresh = 0, key = key)
            pf_loglik_arr.append(pf_val)
            elapsed_time1_arr.append(time.time() - start_time)

            start_time2 = time.time()
            mop_val = -mop(J = J_val,
	        rinit = rinit, rprocess = rprocess, dmeasure = dmeasure,
		theta = theta, ys = ys, alpha = 0.9, key = key)
            mop_loglik_arr.append(mop_val)
            elapsed_time2_arr.append(time.time() - start_time2)
        
        pf_loglik_arr = np.array(pf_loglik_arr)
        mop_loglik_arr = np.array(mop_loglik_arr)
        elapsed_time1_arr = np.array(elapsed_time1_arr)
        elapsed_time2_arr = np.array(elapsed_time2_arr)

        NJresults.append({
            'N': N_val, 
            'J': J_val, 
            'pf_loglik': logmeanexp(pf_loglik_arr), 
            'time_pfilter': np.mean(elapsed_time1_arr), 
            'mop_loglik': logmeanexp(mop_loglik_arr), 
            'time_mop': np.mean(elapsed_time2_arr), 
        })
```

```{python}
#| label: results table
#| output: asis
table=tabulate(NJresults, tablefmt="grid", headers="keys")
print(table)
```


### Log Likelihood vs. Particle Number for Particle Filter and MOP

```{python}
#| code-fold: true
df = pd.DataFrame(NJresults)
unique_N = df['N'].unique()
num_panels = len(unique_N)

# Set up the subplots
fig, axes = plt.subplots(nrows=1, ncols=num_panels, figsize=(5 * num_panels, 5), sharey=False)

if num_panels == 1:
    axes = [axes]  # Ensure axes is iterable if there's only one panel

for ax, N_val in zip(axes, unique_N):
    sub_df = df[df['N'] == N_val]
    sub_df = sub_df.sort_values('J')  # Ensure lines connect in correct order

    ax.plot(sub_df['J'], sub_df['pf_loglik'], label='pf_loglik', marker='o')
    ax.plot(sub_df['J'], sub_df['mop_loglik'], label='mop_loglik', marker='s')

    ax.set_title(f'N = {N_val}')
    ax.set_xlabel('J')
    ax.set_ylabel('Log-likelihood')
    ax.legend()
    ax.grid(True)

plt.tight_layout()
plt.show()
```


### Runtime vs. Particle Number J for Particle Filter and MOP

```{python}
#| code-fold: true

plt.figure(figsize=(8, 6))
for N_val in unique_N:
    sub_df = df[df['N'] == N_val]
    plt.plot(sub_df['J'], sub_df['time_pfilter'], label=f'Particle Filter (N={N_val})', marker='o')
    plt.plot(sub_df['J'], sub_df['time_mop'], label=f'MOP (N={N_val})', marker='x')

plt.xscale('log')
plt.yscale('log')
plt.xlabel('Number of Particles (J)', fontsize=12)
plt.ylabel('Runtime (seconds)', fontsize=12)
plt.title('Runtime vs Particle Number for Different N', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, which="both", linestyle="--", linewidth=0.5)
plt.show()
```

memory requirement scales (J,N) for autodiff? - How much RAM we use


### Comparison of the negative loglikelihood distribution of pfilter on Pypomp and Rpomp when J is small.

```{python}
#| label: small J comparison

J = 10
nrep = N_reps
pypf_loglik_arr = []
xs, ys, key = generate_data(N, key)
for i in range(nrep):
    key, subkey = jax.random.split(key)
    pf_val = -pfilter(J = J,
        rinit = rinit, rprocess = rprocess, dmeasure = dmeasure,
	theta = theta, ys = ys, thresh = 2, key=subkey)
    pypf_loglik_arr.append(pf_val)
    
plt.hist(pypf_loglik_arr, bins=30, edgecolor='black')
plt.title("Distribution of Data")
plt.xlabel("Value")
plt.ylabel("Frequency")
# plt.show()
```




