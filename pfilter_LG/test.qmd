---
title: 'Test the Linear Gaussian Model: A comparison with Kalman-filtering algorithm'
jupyter: python3
---


```{python}
#| label: imports
import jax
import time
import pypomp
import unittest
import tracemalloc
import jax.numpy as np
import numpy as onp
import pandas as pd

from tabulate import tabulate

import pykalman
import seaborn as sns
import matplotlib.pyplot as plt
import jax.scipy.special

from tqdm import tqdm
from pypomp.mop import mop
from pypomp.pfilter import pfilter
from pypomp.fit import fit
from pypomp.internal_functions import _mop_internal
from pypomp.internal_functions import _pfilter_internal
from pypomp.internal_functions import _pfilter_internal_mean
from pypomp.internal_functions import _fit_internal
from pypomp.internal_functions import _perfilter_internal

```

```{python}
#| label: run_level
run_level = 0
N = [10, 50, 100][run_level]
J = [10,1000,10000][run_level]
N_reps = [3,10,100][run_level]
N_values = [[10,50],[100,500],[1000,5000]][run_level]
J_values = [[5,10,20,50],[5,10,100,1000],[10,100,1000,10000]][run_level]
```

Model Setup

Kalman filter deals with the dynamic system:
\begin{align}
    x_t &= A x_{t-1} + w_t \\
    \text{where} \quad
    x_t &\text{ is the current state vector.} \nonumber \\
    A &\text{ is the state transition matrix.} \nonumber \\
    w_t &\sim \mathcal{N}(0, Q) \text{ is the process noise, normally distributed with mean 0 and covariance } Q. \nonumber \\
    y_t &= C x_t + v_t \\
    \text{where} \quad
    y_t &\text{ is the current observation vector.} \nonumber \\
    C &\text{ is the observation matrix.} \nonumber \\
    v_t &\sim \mathcal{N}(0, R) \text{ is the observation noise, normally distributed with mean 0 and covariance} R. \nonumber
\end{align}


(1) Set N (length of time series) to be `{python} N` and generate linear Gaussian states and observations:

```{python}
#| label: LG-POMP model
def get_thetas(theta):
    A = theta[0:4].reshape(2, 2)
    C = theta[4:8].reshape(2, 2)
    Q = theta[8:12].reshape(2, 2)
    R = theta[12:16].reshape(2, 2)
    return A, C, Q, R

def transform_thetas(A, C, Q, R):
    return np.concatenate([A.flatten(), C.flatten(), Q.flatten(), R.flatten()])


fixed = False
key = jax.random.PRNGKey(111)
angle = 0.2
angle2 = angle if fixed else -0.5
A = np.array([[np.cos(angle2), -np.sin(angle)],
             [np.sin(angle), np.cos(angle2)]])
C = np.eye(2)
Q = np.array([[1, 1e-4],
             [1e-4, 1]]) # 100
R = np.array([[1, .1],
            [.1, 1]]) #/ 10
     
theta = transform_thetas(A, C, Q, R)

def generate_data(N, key):
    xs = []
    ys = []
    x = np.ones(2)
    for i in tqdm(range(N)):
        key, subkey = jax.random.split(key)
        x = jax.random.multivariate_normal(key=subkey, mean=A @ x, cov=Q)
        key, subkey = jax.random.split(key)
        y = jax.random.multivariate_normal(key=subkey, mean=C @ x, cov=R)
        xs.append(x)
        ys.append(y)
    xs = np.array(xs)
    ys = np.array(ys)
    return xs, ys, key


def custom_rinit(theta, J, covars=None):
    return np.ones((J, 2))


def custom_rproc(state, theta, key, covars=None):
    A, C, Q, R = get_thetas(theta)
    key, subkey = jax.random.split(key)
    return jax.random.multivariate_normal(key=subkey,
                                          mean=A @ state, cov=Q)
					  

def custom_dmeas(y, preds, theta):
    A, C, Q, R = get_thetas(theta)
    return jax.scipy.stats.multivariate_normal.logpdf(y, preds, R)


rinit = custom_rinit
rproc = custom_rproc
dmeas = custom_dmeas
rprocess = jax.vmap(custom_rproc, (0, None, 0, None))
dmeasure = jax.vmap(custom_dmeas, (None, 0, None))
rprocesses = jax.vmap(custom_rproc, (0, 0, 0, None))
dmeasures = jax.vmap(custom_dmeas, (None, 0, 0))
```

```{python}
#| label: parameter transformation
def logmeanexp(x):
   x_array = np.array(x)
   x_max = np.max(x_array)
   log_mean_exp = np.log(np.mean(np.exp(x_array - x_max))) + x_max
   return log_mean_exp
   
```

Set J=`{python} J` particles and compare the estimated log-likelihood between Kalman filtering and the log-mean-exponential computed over `{python} N_reps` replications for various methods, including classical particle filtering and MOP, etc.

```{python}
#| label: sim and pykalman
xs, ys, key = generate_data(N, key)
kf = pykalman.KalmanFilter(
    transition_matrices=A, observation_matrices=C, 
    transition_covariance=Q, observation_covariance=R)
print("kf loglik =", kf.loglikelihood(ys))
```

```{python}
#| label: pfilter test
loglike = []
for i in range(N_reps):  
    key, subkey = jax.random.split(key)
    pfilter_val = -_pfilter_internal(
        theta, ys, J = J, rinit = rinit,
	rprocess = rprocess, dmeasure = dmeasure,
	covars = None, key= subkey, thresh = -1)
    loglike.append(pfilter_val)

loglike_ = np.array(loglike)

print("Logmeanexp of Particle Filtering =", logmeanexp(loglike))
print("difference between Kalman-Filtering and logmeanexp of Particle Filtering =",
    kf.loglikelihood(ys) - (logmeanexp(loglike)))
```

By calculating the difference between Kalman-filtering and Paticle Filtering algorithm, we discovered that the value of difference can be less than 0.1, indicating that we get a reasonable inference result from the pfilter algorithm.

Next, we test the ouput of the MOP algorithm. The pypomp MOP algorithm is set to be $\phi = \theta$. Under this case, the MOP algorithm should be equivalent with the particle filter algorithm, and have the same output values when setting the same random seeds.

```{python}
#| label: test MOP at different alphas
#| output: asis
alphas = [0, 0.1, 0.3, 0.6, 0.9, 1]
results = []


key = jax.random.PRNGKey(0)  # Use a fixed seed for reproducibility
subkeys = jax.random.split(key, 100)  # Pre-generate 100 keys

for alpha in alphas:
    loglike_mop = []
    for i, subkey in enumerate(subkeys):  
        mop_val = -_mop_internal(theta, ys, J=J, rinit=rinit,
	    rprocess=rprocess, dmeasure=dmeasure,
	    covars=None, key=subkey, alpha=alpha)
        loglike_mop.append(mop_val)
    loglike_mop = np.array(loglike_mop)
    logmeanexp_val = logmeanexp(loglike_mop)
    difference = kf.loglikelihood(ys) - logmeanexp_val
    
    results.append((alpha, logmeanexp_val, difference))
#    print(f"Alpha: {alpha}, Logmeanexp: {logmeanexp_val}, Difference: {difference}")


# Use the same random key to test the particle filter output
loglike_pf = []
for i, subkey in enumerate(subkeys):  
    pfilter_val = -_pfilter_internal(
        theta, ys, J = J, rinit = rinit,
	rprocess = rprocess, dmeasure = dmeasure,
	covars = None, key= subkey, thresh = -1)
    loglike_pf.append(pfilter_val)


#loglike_pf = np.array(loglike_pf)
#print("Logmeanexp of Particle Filtering =", logmeanexp(loglike))
#print("difference between Kalman-Filtering and logmeanexp of Particle Filtering =", kf.loglikelihood(ys) - (logmeanexp(loglike)))

#print("\n")

alpha_table=tabulate(results, tablefmt="grid", headers=("alpha","pf","kalman-pf"))
#alpha_table=tabulate(results, tablefmt="grid")
print(alpha_table)
```

Consistency: For $\alpha = 0$ and $\alpha = 1$, the MOP logmeanexp is `{python} print(round(results[0][1],3))` and `{python} print(round(results[5][1],3))` respectively.
For intermediate $\alpha$ values, the MOP logmeanexp slightly deviates to `{python} print(round(results[3][1],3))`
The Logmeanexp under $\alpha = 0$ and $\alpha = 1$ are closer to the Kalman-Filtering results and the particle filter logmeanexp outputs.


### Test the Linear Gaussian Model: How estimate logllikehood difference and running time varys among different N and J

```{python}
#| label: N-J-test

NJresults = []
key = jax.random.PRNGKey(112)

for N_val in N_values:
    for J_val in J_values:
        print(f"Running with N={N_val}, J={J_val}...")
        
        xs, ys, key = generate_data(N_val, key)
        pf_loglik_arr = []
        mop_loglik_arr = []
        elapsed_time1_arr = []
        elapsed_time2_arr = []
        
        for i in range(N_reps):  
            start_time = time.time()
            pf_val = -pfilter(J = J_val,
	        rinit = rinit, rprocess = rprocess, dmeasure = dmeasure,
		theta = theta, ys = ys, thresh = 0, key = key)
            pf_loglik_arr.append(pf_val)
            elapsed_time1_arr.append(time.time() - start_time)

            start_time2 = time.time()
            mop_val = -mop(J = J_val,
	        rinit = rinit, rprocess = rprocess, dmeasure = dmeasure,
		theta = theta, ys = ys, alpha = 0.9, key = key)
            mop_loglik_arr.append(mop_val)
            elapsed_time2_arr.append(time.time() - start_time2)
        
        pf_loglik_arr = np.array(pf_loglik_arr)
        mop_loglik_arr = np.array(mop_loglik_arr)
        elapsed_time1_arr = np.array(elapsed_time1_arr)
        elapsed_time2_arr = np.array(elapsed_time2_arr)

        NJresults.append({
            'N': N_val, 
            'J': J_val, 
            'pf_loglik': logmeanexp(pf_loglik_arr), 
            'time_pfilter': np.mean(elapsed_time1_arr), 
            'mop_loglik': logmeanexp(mop_loglik_arr), 
            'time_mop': np.mean(elapsed_time2_arr), 
        })
```

```{python}
#| label: results table
#| output: asis
table=tabulate(NJresults, tablefmt="grid", headers="keys")
print(table)
```

## Result (To Be Continued):

```{python}
J_values = [10, 100, 1000, 10000, 100000]
pf_loglik_1000 = [-4059.384, -3766.4568, -3747.1902, -3724.4568, -3797.8877]
mop_loglik_1000 = [-4044.6467, -3776.5305, -3744.5942, -3724.7112, -3797.6775]
time_pf_1000 = [0.00262911, 0.03207133, 0.3003823, 2.2342062, 17.238829]
time_mop_1000 = [0.01157162, 0.0302381, 0.4241002, 3.1925905, 16.385502]

time_pf_5000 = [0.02121412, 0.09896883, 1.323793, 10.692378]
time_mop_5000 = [0.02167264, 0.10377918, 1.9390032, 15.778171]
J_values_5000 = [10, 100, 1000, 10000] 
pf_loglik_5000 = [-20326.46, -18856.922, -18803.223, -18672.562]
mop_loglik_5000 = [-20303.664, -18853.13, -18808.762, -18672.344]
```

```{python}
J_values = [10, 100, 1000, 10000, 100000]
pf_loglik_1000 = [-4097.617, -3777.1707, -3750.4946, -3724.9521, -3797.8877]
mop_loglik_1000 = [-4129.0166, -3770.096, -3747.1416, -3725.1648, -3797.6775]
time_pf_1000 = [0.00325535, 0.02117351, 0.27665812, 2.4753604, 17.238829]
time_mop_1000 = [0.00307242, 0.02399091, 0.40396369, 3.5858035, 16.385502]

J_values_5000 = [10, 100, 1000, 10000]
pf_loglik_5000 = [-20440.643, -18821.348, -18762.62, -18670.256]
mop_loglik_5000 = [-20328.13, -18819.787, -18768.303, -18668.377]
time_pf_5000 = [0.02104483, 0.09821754, 1.4604919, 12.61476]
time_mop_5000 = [0.02136297, 0.10306332, 2.129003, 18.95642]
```

### Log Likelihood vs. Particle Number for Particle Filter and MOP (N=1000,5000)

```{python}
plt.figure(figsize=(8, 6))
plt.plot(J_values, pf_loglik_1000, label='Particle Filter (N=1000)', marker='o')
plt.plot(J_values, mop_loglik_1000, label='MOP (N=1000)', marker='x')
plt.xscale('log')
plt.xlabel('Number of Particles (J)', fontsize=12)
plt.ylabel('Log Likelihood', fontsize=12)
plt.title('Log Likelihood vs Particle Number (N=1000)', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, which="both", linestyle="--", linewidth=0.5)
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
plt.plot(J_values_5000, pf_loglik_5000, label='Particle Filter (N=5000)', marker='o')
plt.plot(J_values_5000, mop_loglik_5000, label='MOP (N=5000)', marker='x')
plt.xscale('log')
plt.xlabel('Number of Particles (J)', fontsize=12)
plt.ylabel('Log Likelihood', fontsize=12)
plt.title('Log Likelihood vs Particle Number (N=5000)', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, which="both", linestyle="--", linewidth=0.5)
plt.show()
```

### Runtime vs. Particle Number J for Particle Filter and MOP (N=1000, 5000)

```{python}

plt.figure(figsize=(8, 6))
plt.plot(J_values, time_pf_1000, label='Particle Filter (N=1000)', marker='o')
plt.plot(J_values, time_mop_1000, label='MOP (N=1000)', marker='x')
plt.plot(J_values_5000, time_pf_5000, label='Particle Filter (N=5000)', marker='o', linestyle='--')
plt.plot(J_values_5000, time_mop_5000, label='MOP (N=5000)', marker='x', linestyle='--')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('Number of Particles (J)', fontsize=12)
plt.ylabel('Runtime (seconds)', fontsize=12)
plt.title('Runtime vs Particle Number for Different N', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, which="both", linestyle="--", linewidth=0.5)
plt.show()
```

memory requirement scales (J,N) for autodiff? - How much RAM we use


### Comparison of the negative loglikelihood distribution of pfilter on Pypomp and Rpomp when J is small.

```{python}
#| label: small J comparison

J = 10
nrep = N_reps
pypf_loglik_arr = []
xs, ys, key = generate_data(N, key)
for i in range(nrep):
    key, subkey = jax.random.split(key)
    pf_val = -pfilter(J = J,
        rinit = rinit, rprocess = rprocess, dmeasure = dmeasure,
	theta = theta, ys = ys, thresh = 2, key=subkey)
    pypf_loglik_arr.append(pf_val)
    
plt.hist(pypf_loglik_arr, bins=30, edgecolor='black')
plt.title("Distribution of Data")
plt.xlabel("Value")
plt.ylabel("Frequency")
# plt.show()
```




